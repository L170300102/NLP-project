{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Honor Project: Custom Conversational Model\n",
    "- cornell dataset is used to train the model\n",
    "- GloVe embeddings 300D is used\n",
    "- LSTM + attention model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "Data fields are separated by \" +++$+++ \"\n",
    "- `movie_lines.txt`:<br>\n",
    "    - lineId\n",
    "    - characterId\n",
    "    - movieId\n",
    "    - character name\n",
    "    - text of utterance\n",
    "            \n",
    "- `movie_conversations.txt`:<br>\n",
    "    - characterID of the first character involved in the conversation\n",
    "    - characterID of the second character involved in the conversation\n",
    "    - movieId\n",
    "    - list of the utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in datasets\n",
    "conversations_df = open('data/cornell/movie_conversations.txt').read().split('\\n')\n",
    "conversations_df = [line.split(' +++$+++ ') for line in conversations_df]\n",
    "\n",
    "lines_df = open('data/cornell/movie_lines.txt', encoding='utf8', errors='ignore').read().split('\\n')\n",
    "lines_df = [line.split(' +++$+++ ') for line in lines_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dictionary to convert id to line\n",
    "id2line = {}\n",
    "for line in lines_df:\n",
    "    if len(line)==5:\n",
    "        id2line[line[0]] = line[4]\n",
    "        \n",
    "# list of utterances id\n",
    "conversations = []\n",
    "for line in conversations_df:\n",
    "    line = line[-1][1:-1]\n",
    "    line = line.replace(\"'\", \"\").split(', ')\n",
    "    conversations.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question, answer\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for conv in conversations:\n",
    "    for i in range(len(conv)-1):\n",
    "        questions.append(id2line[conv[i]])\n",
    "        answers.append(id2line[conv[i+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
      "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "\n",
      "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "Not the hacking and gagging and spitting part.  Please.\n",
      "\n",
      "Not the hacking and gagging and spitting part.  Please.\n",
      "Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
      "\n",
      "You're asking me out.  That's so cute. What's your name again?\n",
      "Forget it.\n",
      "\n",
      "No, no, it's my fault -- we didn't have a proper introduction ---\n",
      "Cameron.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_num = 5\n",
    "for i in range(display_num):\n",
    "    print(questions[i])\n",
    "    print(answers[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output questions, answers\n",
    "questions_path = 'data/cornell/questions.txt'\n",
    "answers_path = 'data/cornell/answers.txt'\n",
    "\n",
    "out = open(questions_path, 'w')\n",
    "for line in questions:\n",
    "    print(line, sep='\\t', file=out)\n",
    "out.close()\n",
    "\n",
    "out = open(answers_path, 'w')\n",
    "for line in answers:\n",
    "    print(line, sep='\\t', file=out)\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe Embeddings\n",
    "glove.6B.300d is used in this model. We should get the vocabulary as close to the embeddings as possible. Let's create vocabulary from raw train data and check the intersection between our vocabulary and the embeddins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "with open('data/cornell/questions.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        questions.append(line.strip())\n",
    "    \n",
    "answers = []\n",
    "with open('data/cornell/answers.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        answers.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the datasets to train, test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "questions_train, questions_test, answers_train, answers_test = train_test_split(questions, answers, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the data using TweetTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "tokenized_questions = [tokenizer.tokenize(sentence) for sentence in questions_train]\n",
    "tokenized_answers = [tokenizer.tokenize(sentence) for sentence in answers_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"He's\", 'in', 'Jamaica', 'with', 'a', 'twenty-three', '-', 'year-old', '.', 'A', 'friend', 'of', 'my', \"daughter's\", '.', 'He', 'had', 'the', 'fucking', 'nerve', 'to', 'call', 'me', 'and', 'ask', 'me', 'to', 'borrow', 'some', 'money', 'and', 'I', 'told', 'him', 'to', 'fuck', 'off', ',', 'so', 'he', 'asked', 'me', 'to', 'sell', 'his', 'singles', 'collection', 'and', 'send', 'him', 'a', 'check', 'for', 'whatever', 'I', 'go', ',', 'minus', 'a', 'ten', 'percent', 'commission', '.', 'Which', 'reminds', 'me', '.', 'Can', 'you', 'make', 'sure', 'you', 'give', 'me', 'a', 'five', '?', 'I', 'want', 'to', 'frame', 'it', 'and', 'put', 'it', 'on', 'the', 'wall', '.']\n",
      "['It', 'must', 'have', 'taken', 'him', 'a', 'long', 'time', 'to', 'get', 'them', 'together', '.']\n",
      "\n",
      "['The', 'one', 'where', 'you', 'go', 'to', 'the', 'slave', 'market', '.', 'You', 'can', 'cut', 'right', 'to', 'the', 'scene', 'where', 'John', 'the', 'Baptist', '-']\n",
      "['Cut', 'away', 'from', 'me', '?']\n",
      "\n",
      "['What', '?', 'Why', '?']\n",
      "['The', 'theological', 'ramifications', 'of', 'all', 'this', 'are', 'obvious', ';', 'the', 'President', 'feels', 'we', 'need', 'to', 'include', 'religious', 'interests', 'rather', 'than', 'alienate', 'them', '.', \"She's\", 'also', 'named', 'Palmer', 'Joss', 'as', 'their', 'liaison', ';', \"he's\", 'requested', 'a', 'meeting', 'with', 'you', '.']\n",
      "\n",
      "['What', 'will', 'we', 'do', '?']\n",
      "[\"'\", 'Love', 'will', 'find', 'out', 'the', 'way', \"'\", '.']\n",
      "\n",
      "[\"How'd\", 'you', 'people', 'happen', 'to', 'pop', 'in', '?']\n",
      "['We', 'hear', 'this', 'is', 'getting', 'to', 'be', 'sort', 'of', 'a', 'meeting', 'place', 'for', 'the', 'Wynant', 'family', ',', 'so', 'we', 'figure', \"we'll\", 'stick', 'around', 'in', 'case', 'the', 'old', 'man', 'himself', 'shows', 'up', '.', 'Then', 'we', 'seen', 'him', '...  ...', 'sneak', 'in', 'and', 'we', 'decide', 'to', 'come', 'up', '.', 'And', 'pretty', 'lucky', 'for', 'you', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_num = 5\n",
    "\n",
    "for i in range(display_num):\n",
    "    print(tokenized_questions[i])\n",
    "    print(tokenized_answers[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary with number of occurances\n",
    "vocab_occ = {}\n",
    "for dataset in [tokenized_questions, tokenized_answers]:\n",
    "    for sentence in dataset:\n",
    "        for word in sentence:\n",
    "            vocab_occ[word] = vocab_occ.get(word, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load GloVe\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# convert GloVe vectors into the word2vec\n",
    "glove_file = 'glove.6B.300d.txt'\n",
    "tmp_file = 'glove_word2vec.txt'\n",
    "glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "embeddings = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def check_coverage(vocab, embeddings):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    k = 0\n",
    "    i = 0\n",
    "    for word in vocab:\n",
    "        try:\n",
    "            a[word] = embeddings[word]\n",
    "            k += vocab[word]\n",
    "        except:\n",
    "            oov[word] = vocab[word]\n",
    "            i += vocab[word]\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(a)/len(vocab)))\n",
    "    print('Found embeddings for {:.2%} of all texts'.format(k/(k+i)))\n",
    "    \n",
    "    sorted_oov = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "    \n",
    "    return sorted_oov          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 44.14% of vocab\n",
      "Found embeddings for 78.92% of all texts\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab_occ, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 44.23% of vocabulary has embeddings in GloVe. Let's take a look at oov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 136746),\n",
       " ('You', 36536),\n",
       " (\"I'm\", 29491),\n",
       " (\"don't\", 26419),\n",
       " ('What', 22480),\n",
       " ('No', 15840),\n",
       " (\"It's\", 13896),\n",
       " ('The', 13891),\n",
       " ('And', 13490),\n",
       " ('But', 10886),\n",
       " ('Well', 9876),\n",
       " ('Oh', 9683),\n",
       " (\"you're\", 9571),\n",
       " (\"it's\", 9508),\n",
       " ('He', 9373)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of oov is capital letter. Let's convert all letter into lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab of lowercase\n",
    "vocab_occ = {}\n",
    "for dataset in [tokenized_questions, tokenized_answers]:\n",
    "    for sentence in dataset:\n",
    "        for word in sentence:\n",
    "            word = word.lower()\n",
    "            vocab_occ[word] = vocab_occ.get(word, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 71.05% of vocab\n",
      "Found embeddings for 94.22% of all texts\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab_occ, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"don't\", 32074),\n",
       " (\"i'm\", 29564),\n",
       " (\"it's\", 23451),\n",
       " (\"you're\", 17785),\n",
       " (\"that's\", 14573),\n",
       " (\"can't\", 8681),\n",
       " (\"i'll\", 8621),\n",
       " (\"he's\", 8593),\n",
       " (\"didn't\", 7907),\n",
       " (\"i've\", 6990),\n",
       " (\"what's\", 6155),\n",
       " (\"we're\", 5721),\n",
       " (\"there's\", 5028),\n",
       " ('</u>', 4494),\n",
       " ('<u>', 4491)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "71.10% of vocabulary has embeddings now.<br>\n",
    "Most of oov has n't, 'd, 've. Let's try nltk.word_tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "tokenized_questions = [nltk.word_tokenize(sentence) for sentence in questions_train]\n",
    "tokenized_answers = [nltk.word_tokenize(sentence) for sentence in answers_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab of lowercase\n",
    "vocab_occ = {}\n",
    "for dataset in [tokenized_questions, tokenized_answers]:\n",
    "    for sentence in dataset:\n",
    "        for word in sentence:\n",
    "            word = word.lower()\n",
    "            vocab_occ[word] = vocab_occ.get(word, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 74.34% of vocab\n",
      "Found embeddings for 99.26% of all texts\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab_occ, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "74.33% now! Let's start building model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output questions_train, answers_train, questions_test, answers_test\n",
    "questions_train_path = 'data/cornell/questions_train.txt'\n",
    "answers_train_path = 'data/cornell/answers_train.txt'\n",
    "questions_test_path = 'data/cornell/questions_test.txt'\n",
    "answers_test_path = 'data/cornell/answers_test.txt'\n",
    "\n",
    "dataset_list = [questions_train, answers_train, questions_test, answers_test]\n",
    "path_list = [questions_train_path, answers_train_path, questions_test_path, answers_test_path]\n",
    "\n",
    "for dataset, path in zip(dataset_list, path_list):\n",
    "    out = open(path, 'w')\n",
    "    for line in dataset:\n",
    "        print(line, sep='\\t', file=out)\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "Datasets should be tokenized by nltk.word_tokenize, then lowercased.<br>\n",
    "After that, OOV should be replaced by <UNK> token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_train_path = 'data/cornell/questions_train.txt'\n",
    "answers_train_path = 'data/cornell/answers_train.txt'\n",
    "questions_test_path = 'data/cornell/questions_test.txt'\n",
    "answers_test_path = 'data/cornell/answers_test.txt'\n",
    "\n",
    "questions_train, answers_train, questions_test, answers_test = [], [], [], []\n",
    "\n",
    "dataset_list = [questions_train, answers_train, questions_test, answers_test]\n",
    "path_list = [questions_train_path, answers_train_path, questions_test_path, answers_test_path]\n",
    "\n",
    "for dataset, path in zip(dataset_list, path_list):\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            dataset.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_symbol = '<S>'\n",
    "end_symbol = '</S>'\n",
    "padding_symbol = '<PAD>'\n",
    "unknown_symbol = '<UNK>'\n",
    "\n",
    "special_symbols = [start_symbol, end_symbol, padding_symbol, unknown_symbol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):               \n",
    "    text = re.sub(r\"<[^>]*>\", \"\", text)\n",
    "    text = re.sub(r\"[<>]\", \"\", text)\n",
    "                               \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset):\n",
    "    preprocessed_dataset = []\n",
    "    for sentence in dataset:\n",
    "        cleaned_sentence = clean_text(sentence)\n",
    "        tokenized_sentence = nltk.word_tokenize(cleaned_sentence)\n",
    "        final_sentence = [word.lower() for word in tokenized_sentence]\n",
    "        preprocessed_dataset.append(final_sentence)\n",
    "        \n",
    "    return preprocessed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "tokenized_questions_train = preprocess_dataset(questions_train)\n",
    "tokenized_answers_train = preprocess_dataset(answers_train)\n",
    "\n",
    "# test set\n",
    "tokenized_questions_test = preprocess_dataset(questions_test)\n",
    "tokenized_answers_test = preprocess_dataset(answers_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check occurances of words\n",
    "vocab_occ = {}\n",
    "for dataset in [tokenized_questions_train, tokenized_answers_train]:\n",
    "    for sentence in dataset:\n",
    "        for word in sentence:\n",
    "            vocab_occ[word] = vocab_occ.get(word, 0) + 1\n",
    "vocab_occ = sorted(vocab_occ.items(), key=lambda kv: kv[1])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 435695),\n",
       " (',', 217326),\n",
       " ('you', 193685),\n",
       " ('i', 185806),\n",
       " ('?', 147218),\n",
       " ('the', 126954),\n",
       " ('to', 104712),\n",
       " ('a', 92028),\n",
       " ('it', 85906),\n",
       " (\"'s\", 85811),\n",
       " (\"n't\", 72931),\n",
       " ('...', 65942),\n",
       " ('do', 62397),\n",
       " ('that', 61177),\n",
       " ('and', 59657),\n",
       " ('of', 50743),\n",
       " ('what', 50277),\n",
       " ('!', 45558),\n",
       " ('in', 44176),\n",
       " ('me', 42152),\n",
       " ('is', 40728),\n",
       " ('we', 36633),\n",
       " ('he', 35769),\n",
       " ('--', 34923),\n",
       " ('this', 30758),\n",
       " ('for', 30332),\n",
       " ('have', 29847),\n",
       " (\"'m\", 29771),\n",
       " ('know', 28871),\n",
       " ('was', 28139),\n",
       " (\"'re\", 28127),\n",
       " ('your', 27037),\n",
       " ('my', 26879),\n",
       " ('not', 26590),\n",
       " ('no', 26048),\n",
       " ('be', 25114),\n",
       " ('on', 24964),\n",
       " ('but', 23009),\n",
       " ('with', 22591),\n",
       " ('are', 22534),\n",
       " ('they', 22111),\n",
       " ('just', 20637),\n",
       " ('like', 19609),\n",
       " ('all', 19596),\n",
       " ('did', 19357),\n",
       " ('about', 18665),\n",
       " ('there', 18608),\n",
       " (\"'ll\", 18382),\n",
       " ('get', 18054),\n",
       " ('so', 17363),\n",
       " ('if', 17167),\n",
       " ('got', 17051),\n",
       " ('out', 17040),\n",
       " ('here', 16278),\n",
       " ('she', 16203),\n",
       " ('him', 15629),\n",
       " ('how', 14976),\n",
       " ('up', 14970),\n",
       " ('can', 14934),\n",
       " ('want', 14629),\n",
       " ('think', 14189),\n",
       " (\"'ve\", 14049),\n",
       " ('at', 13868),\n",
       " ('one', 13652),\n",
       " ('well', 13029),\n",
       " ('right', 12693),\n",
       " ('go', 12566),\n",
       " ('now', 12541),\n",
       " (\"'\", 12470),\n",
       " ('her', 12220),\n",
       " ('why', 12072),\n",
       " ('going', 11293),\n",
       " ('who', 11162),\n",
       " ('would', 10974),\n",
       " ('see', 10373),\n",
       " ('yes', 10328),\n",
       " ('as', 10175),\n",
       " ('oh', 10145),\n",
       " (\"'d\", 9653),\n",
       " ('yeah', 9615),\n",
       " ('his', 9526),\n",
       " ('could', 9507),\n",
       " ('when', 9481),\n",
       " ('good', 9271),\n",
       " ('an', 9178),\n",
       " ('were', 9062),\n",
       " ('tell', 8974),\n",
       " ('from', 8930),\n",
       " ('where', 8776),\n",
       " ('ca', 8690),\n",
       " ('or', 8606),\n",
       " ('time', 8534),\n",
       " ('been', 8517),\n",
       " ('come', 8422),\n",
       " ('some', 8377),\n",
       " ('will', 8307),\n",
       " ('-', 8135),\n",
       " ('say', 8108),\n",
       " ('then', 8029),\n",
       " ('them', 7901),\n",
       " ('back', 7791),\n",
       " ('had', 7698),\n",
       " ('let', 7547),\n",
       " ('man', 7519),\n",
       " ('look', 7379),\n",
       " ('us', 7276),\n",
       " ('never', 7229),\n",
       " ('take', 7136),\n",
       " ('something', 7130),\n",
       " ('mean', 7028),\n",
       " ('na', 6793),\n",
       " (\"''\", 6584),\n",
       " ('``', 6548),\n",
       " ('too', 6435),\n",
       " ('way', 6376),\n",
       " ('does', 6319),\n",
       " ('really', 6119),\n",
       " ('make', 6106),\n",
       " ('little', 5846),\n",
       " ('sure', 5811),\n",
       " ('more', 5769),\n",
       " ('okay', 5644),\n",
       " ('down', 5627),\n",
       " ('any', 5575),\n",
       " ('gon', 5514),\n",
       " ('people', 5474),\n",
       " ('should', 5399),\n",
       " ('said', 5359),\n",
       " ('thing', 5355),\n",
       " ('over', 5258),\n",
       " ('by', 5251),\n",
       " ('only', 5243),\n",
       " ('need', 5232),\n",
       " ('has', 5202),\n",
       " ('maybe', 5180),\n",
       " ('our', 5108),\n",
       " ('very', 5085),\n",
       " ('off', 5057),\n",
       " ('much', 4787),\n",
       " ('two', 4664),\n",
       " ('anything', 4544),\n",
       " ('am', 4485),\n",
       " ('because', 4448),\n",
       " ('give', 4444),\n",
       " ('even', 4280),\n",
       " ('nothing', 4261),\n",
       " ('talk', 4227),\n",
       " ('sorry', 4181),\n",
       " ('love', 4150),\n",
       " ('thought', 4108),\n",
       " ('mr.', 4078),\n",
       " ('into', 3973),\n",
       " ('doing', 3938),\n",
       " ('life', 3864),\n",
       " ('sir', 3851),\n",
       " ('call', 3823),\n",
       " ('ever', 3810),\n",
       " ('before', 3806),\n",
       " ('money', 3742),\n",
       " ('other', 3689),\n",
       " ('find', 3680),\n",
       " ('told', 3677),\n",
       " ('things', 3671),\n",
       " ('still', 3664),\n",
       " ('night', 3648),\n",
       " ('work', 3636),\n",
       " ('around', 3614),\n",
       " ('first', 3574),\n",
       " ('must', 3551),\n",
       " ('than', 3520),\n",
       " ('these', 3511),\n",
       " ('better', 3494),\n",
       " ('guy', 3485),\n",
       " ('last', 3412),\n",
       " ('wo', 3365),\n",
       " ('long', 3346),\n",
       " ('help', 3340),\n",
       " ('believe', 3331),\n",
       " ('those', 3331),\n",
       " ('always', 3325),\n",
       " ('years', 3255),\n",
       " ('after', 3227),\n",
       " ('day', 3200),\n",
       " ('put', 3199),\n",
       " ('away', 3182),\n",
       " ('god', 3137),\n",
       " ('please', 3115),\n",
       " ('name', 3062),\n",
       " ('old', 3061),\n",
       " ('shit', 3050),\n",
       " ('again', 3031),\n",
       " ('their', 3019),\n",
       " ('place', 3009),\n",
       " ('hey', 2958),\n",
       " ('keep', 2915),\n",
       " ('feel', 2905),\n",
       " ('everything', 2903),\n",
       " ('home', 2872),\n",
       " ('big', 2870),\n",
       " ('new', 2855),\n",
       " ('great', 2826),\n",
       " ('hell', 2820),\n",
       " ('kind', 2732),\n",
       " ('bad', 2716),\n",
       " ('remember', 2660),\n",
       " ('lot', 2643),\n",
       " ('course', 2630),\n",
       " ('fuck', 2630),\n",
       " ('understand', 2606),\n",
       " ('talking', 2594),\n",
       " ('dead', 2576),\n",
       " ('guess', 2571),\n",
       " ('might', 2556),\n",
       " ('kill', 2535),\n",
       " ('else', 2508),\n",
       " ('made', 2492),\n",
       " ('girl', 2486),\n",
       " ('leave', 2485),\n",
       " ('every', 2475),\n",
       " ('wrong', 2470),\n",
       " ('three', 2450),\n",
       " ('wait', 2450),\n",
       " ('ask', 2422),\n",
       " ('ai', 2414),\n",
       " ('wanted', 2409),\n",
       " ('listen', 2391),\n",
       " ('ta', 2387),\n",
       " ('real', 2383),\n",
       " ('enough', 2381),\n",
       " ('fine', 2371),\n",
       " ('thank', 2370),\n",
       " ('hear', 2368),\n",
       " ('happened', 2353),\n",
       " ('father', 2334),\n",
       " ('someone', 2327),\n",
       " ('stop', 2306),\n",
       " ('through', 2295),\n",
       " ('mind', 2272),\n",
       " ('nice', 2270),\n",
       " ('yourself', 2270),\n",
       " ('done', 2269),\n",
       " ('uh', 2256),\n",
       " ('own', 2244),\n",
       " ('house', 2205),\n",
       " ('another', 2193),\n",
       " ('huh', 2184),\n",
       " ('fucking', 2177),\n",
       " ('getting', 2164),\n",
       " ('world', 2150),\n",
       " ('came', 2147),\n",
       " ('care', 2145),\n",
       " ('job', 2136),\n",
       " ('mother', 2123),\n",
       " ('left', 2107),\n",
       " ('stay', 2095),\n",
       " ('being', 2055),\n",
       " ('saw', 2041),\n",
       " (';', 2039),\n",
       " ('car', 2039),\n",
       " (':', 2038),\n",
       " ('knew', 2021),\n",
       " ('trying', 2010),\n",
       " ('try', 1994),\n",
       " ('boy', 1990),\n",
       " ('same', 1980),\n",
       " ('went', 1973),\n",
       " ('heard', 1955),\n",
       " ('room', 1893),\n",
       " ('friend', 1891),\n",
       " ('thanks', 1880),\n",
       " ('which', 1880),\n",
       " ('whole', 1867),\n",
       " ('seen', 1862),\n",
       " ('looking', 1852),\n",
       " ('live', 1842),\n",
       " ('may', 1833),\n",
       " ('guys', 1831),\n",
       " ('tonight', 1824),\n",
       " ('show', 1812),\n",
       " ('coming', 1805),\n",
       " ('used', 1777),\n",
       " ('business', 1777),\n",
       " ('pretty', 1767),\n",
       " ('best', 1759),\n",
       " ('killed', 1724),\n",
       " (\"'em\", 1713),\n",
       " ('many', 1712),\n",
       " ('matter', 1699),\n",
       " ('idea', 1695),\n",
       " ('ya', 1694),\n",
       " ('wife', 1658),\n",
       " ('says', 1651),\n",
       " ('next', 1649),\n",
       " ('miss', 1645),\n",
       " ('found', 1638),\n",
       " ('called', 1637),\n",
       " ('woman', 1636),\n",
       " ('without', 1630),\n",
       " ('myself', 1618),\n",
       " ('once', 1616),\n",
       " ('five', 1616),\n",
       " ('men', 1613),\n",
       " ('saying', 1588),\n",
       " ('tomorrow', 1581),\n",
       " ('head', 1578),\n",
       " ('while', 1578),\n",
       " ('run', 1561),\n",
       " ('morning', 1555),\n",
       " ('yet', 1542),\n",
       " ('few', 1529),\n",
       " ('most', 1514),\n",
       " ('together', 1509),\n",
       " ('since', 1507),\n",
       " ('already', 1499),\n",
       " ('start', 1499),\n",
       " ('stuff', 1498),\n",
       " ('ago', 1494),\n",
       " ('play', 1473),\n",
       " ('use', 1469),\n",
       " ('hundred', 1466),\n",
       " ('days', 1460),\n",
       " ('probably', 1449),\n",
       " ('somebody', 1447),\n",
       " ('crazy', 1437),\n",
       " ('baby', 1436),\n",
       " ('took', 1431),\n",
       " ('nobody', 1425),\n",
       " ('hard', 1419),\n",
       " ('school', 1410),\n",
       " ('dad', 1397),\n",
       " ('alone', 1395),\n",
       " ('kid', 1395),\n",
       " ('today', 1389),\n",
       " ('son', 1386),\n",
       " ('anyone', 1380),\n",
       " ('four', 1380),\n",
       " ('forget', 1361),\n",
       " ('gone', 1361),\n",
       " ('anyway', 1358),\n",
       " ('such', 1358),\n",
       " ('afraid', 1357),\n",
       " ('meet', 1354),\n",
       " ('wants', 1353),\n",
       " ('case', 1352),\n",
       " ('ten', 1352),\n",
       " ('deal', 1351),\n",
       " ('problem', 1346),\n",
       " ('friends', 1345),\n",
       " ('point', 1336),\n",
       " ('until', 1327),\n",
       " ('exactly', 1326),\n",
       " ('minute', 1321),\n",
       " ('part', 1319),\n",
       " ('thinking', 1300),\n",
       " ('knows', 1288),\n",
       " ('wan', 1281),\n",
       " ('damn', 1265),\n",
       " ('hope', 1261),\n",
       " ('makes', 1247),\n",
       " ('jesus', 1243),\n",
       " ('supposed', 1242),\n",
       " ('bring', 1233),\n",
       " ('doctor', 1231),\n",
       " ('read', 1228),\n",
       " ('die', 1225),\n",
       " ('hello', 1224),\n",
       " ('year', 1219),\n",
       " ('worry', 1216),\n",
       " ('word', 1216),\n",
       " ('happen', 1205),\n",
       " ('true', 1203),\n",
       " ('face', 1201),\n",
       " ('family', 1196),\n",
       " ('minutes', 1195),\n",
       " ('story', 1193),\n",
       " ('looks', 1193),\n",
       " ('everybody', 1191),\n",
       " ('working', 1175),\n",
       " ('pay', 1170),\n",
       " ('mom', 1169),\n",
       " ('six', 1162),\n",
       " ('married', 1161),\n",
       " ('thousand', 1161),\n",
       " ('brother', 1161),\n",
       " ('hurt', 1152),\n",
       " ('week', 1152),\n",
       " ('watch', 1150),\n",
       " ('lost', 1149),\n",
       " ('actually', 1145),\n",
       " ('hold', 1142),\n",
       " ('shut', 1134),\n",
       " ('town', 1134),\n",
       " ('times', 1127),\n",
       " ('under', 1125),\n",
       " ('gave', 1124),\n",
       " ('kids', 1119),\n",
       " ('whatever', 1117),\n",
       " ('move', 1115),\n",
       " ('turn', 1114),\n",
       " ('happy', 1113),\n",
       " ('ass', 1112),\n",
       " ('both', 1111),\n",
       " ('least', 1111),\n",
       " ('john', 1110),\n",
       " ('fuckin', 1106),\n",
       " ('anybody', 1101),\n",
       " ('having', 1101),\n",
       " ('sleep', 1099),\n",
       " ('different', 1096),\n",
       " ('soon', 1095),\n",
       " ('end', 1093),\n",
       " ('hi', 1092),\n",
       " ('late', 1089),\n",
       " ('asked', 1078),\n",
       " ('beautiful', 1077),\n",
       " ('gets', 1077),\n",
       " ('far', 1076),\n",
       " ('mrs.', 1075),\n",
       " ('half', 1074),\n",
       " ('alright', 1071),\n",
       " ('open', 1071),\n",
       " ('trouble', 1068),\n",
       " ('sit', 1064),\n",
       " ('each', 1062),\n",
       " ('jack', 1055),\n",
       " ('phone', 1053),\n",
       " ('mine', 1051),\n",
       " ('hate', 1049),\n",
       " ('drink', 1049),\n",
       " ('young', 1047),\n",
       " ('hit', 1046),\n",
       " ('easy', 1044),\n",
       " ('ready', 1044),\n",
       " ('death', 1042),\n",
       " ('cut', 1037),\n",
       " ('shot', 1034),\n",
       " ('change', 1032),\n",
       " ('police', 1032),\n",
       " ('question', 1021),\n",
       " ('taking', 1020),\n",
       " ('making', 1017),\n",
       " ('wish', 1017),\n",
       " ('rest', 1016),\n",
       " ('couple', 1015),\n",
       " ('eat', 1013),\n",
       " ('chance', 1011),\n",
       " ('quite', 1010),\n",
       " ('dollars', 1004),\n",
       " ('door', 1003),\n",
       " ('later', 1002),\n",
       " ('met', 1002),\n",
       " ('set', 998),\n",
       " ('everyone', 997),\n",
       " ('yours', 997),\n",
       " ('person', 991),\n",
       " ('walk', 990),\n",
       " ('hand', 990),\n",
       " ('trust', 989),\n",
       " ('hours', 983),\n",
       " ('telling', 979),\n",
       " ('close', 979),\n",
       " ('comes', 977),\n",
       " ('anymore', 975),\n",
       " ('second', 975),\n",
       " ('important', 974),\n",
       " ('sometimes', 973),\n",
       " ('truth', 963),\n",
       " ('check', 963),\n",
       " ('goes', 947),\n",
       " ('heart', 943),\n",
       " ('reason', 934),\n",
       " ('dr.', 930),\n",
       " ('funny', 926),\n",
       " ('gun', 924),\n",
       " ('eyes', 923),\n",
       " ('side', 920),\n",
       " ('bit', 917),\n",
       " ('either', 908),\n",
       " ('suppose', 908),\n",
       " ('war', 906),\n",
       " ('tried', 895),\n",
       " ('though', 887),\n",
       " ('inside', 887),\n",
       " ('means', 886),\n",
       " ('against', 885),\n",
       " ('buy', 877),\n",
       " ('honey', 874),\n",
       " ('game', 871),\n",
       " ('its', 869),\n",
       " ('bed', 866),\n",
       " ('number', 863),\n",
       " ('bet', 863),\n",
       " ('answer', 858),\n",
       " ('sort', 857),\n",
       " ('country', 855),\n",
       " ('white', 855),\n",
       " ('body', 855),\n",
       " ('send', 853),\n",
       " ('along', 849),\n",
       " ('women', 846),\n",
       " ('stand', 846),\n",
       " ('party', 845),\n",
       " ('sick', 843),\n",
       " ('water', 842),\n",
       " ('captain', 842),\n",
       " ('office', 836),\n",
       " ('waiting', 829),\n",
       " ('months', 828),\n",
       " ('break', 825),\n",
       " ('almost', 822),\n",
       " ('died', 816),\n",
       " ('fire', 813),\n",
       " ('christ', 807),\n",
       " ('excuse', 806),\n",
       " ('pick', 804),\n",
       " ('husband', 801),\n",
       " ('behind', 797),\n",
       " ('alive', 793),\n",
       " ('book', 793),\n",
       " ('started', 792),\n",
       " ('stupid', 784),\n",
       " ('fact', 783),\n",
       " ('ah', 782),\n",
       " ('himself', 774),\n",
       " ('daddy', 771),\n",
       " ('figure', 771),\n",
       " ('city', 770),\n",
       " ('serious', 767),\n",
       " ('hour', 766),\n",
       " ('twenty', 764),\n",
       " ('million', 763),\n",
       " ('black', 763),\n",
       " ('high', 761),\n",
       " ('running', 760),\n",
       " ('hands', 760),\n",
       " ('living', 757),\n",
       " ('sense', 755),\n",
       " ('between', 751),\n",
       " ('speak', 749),\n",
       " ('goddamn', 747),\n",
       " ('dinner', 745),\n",
       " ('seem', 744),\n",
       " ('line', 743),\n",
       " ('also', 742),\n",
       " ('seems', 741),\n",
       " ('blood', 740),\n",
       " ('street', 735),\n",
       " ('scared', 734),\n",
       " ('power', 724),\n",
       " ('dear', 723),\n",
       " ('boys', 721),\n",
       " ('shoot', 719),\n",
       " ('drive', 717),\n",
       " ('fight', 715),\n",
       " ('frank', 714),\n",
       " ('feeling', 714),\n",
       " ('lady', 713),\n",
       " ('able', 712),\n",
       " ('bullshit', 711),\n",
       " (\"'cause\", 709),\n",
       " ('sounds', 707),\n",
       " ('ahead', 706),\n",
       " ('ship', 706),\n",
       " ('full', 704),\n",
       " ('george', 703),\n",
       " ('news', 702),\n",
       " ('write', 695),\n",
       " ('sent', 690),\n",
       " ('perhaps', 689),\n",
       " ('outside', 689),\n",
       " ('asking', 688),\n",
       " ('happens', 688),\n",
       " ('children', 683),\n",
       " ('safe', 680),\n",
       " ('fun', 678),\n",
       " ('free', 678),\n",
       " ('brought', 675),\n",
       " ('weeks', 673),\n",
       " ('sound', 672),\n",
       " ('lose', 671),\n",
       " ('bill', 670),\n",
       " ('dog', 669),\n",
       " ('front', 666),\n",
       " ('moment', 664),\n",
       " ('promise', 656),\n",
       " ('eight', 655),\n",
       " ('president', 653),\n",
       " ('girls', 652),\n",
       " ('save', 652),\n",
       " ('child', 650),\n",
       " ('ok', 650),\n",
       " ('york', 649),\n",
       " ('cause', 649),\n",
       " ('somewhere', 646),\n",
       " ('poor', 643),\n",
       " ('taken', 640),\n",
       " ('beat', 639),\n",
       " ('sister', 637),\n",
       " (']', 635),\n",
       " ('lives', 635),\n",
       " ('lie', 633),\n",
       " ('hot', 629),\n",
       " ('daughter', 628),\n",
       " ('sex', 624),\n",
       " ('shall', 623),\n",
       " ('glad', 623),\n",
       " ('till', 620),\n",
       " ('plan', 617),\n",
       " ('needs', 617),\n",
       " ('harry', 616),\n",
       " ('light', 615),\n",
       " ('possible', 613),\n",
       " ('thinks', 609),\n",
       " ('movie', 609),\n",
       " ('cool', 608),\n",
       " ('words', 606),\n",
       " ('looked', 603),\n",
       " ('fifty', 601),\n",
       " ('tired', 599),\n",
       " ('small', 599),\n",
       " ('company', 598),\n",
       " ('fast', 595),\n",
       " ('worth', 594),\n",
       " ('wonderful', 594),\n",
       " ('lucky', 593),\n",
       " ('straight', 592),\n",
       " ('piece', 591),\n",
       " ('food', 588),\n",
       " ('leaving', 588),\n",
       " ('cold', 588),\n",
       " ('king', 586),\n",
       " ('picture', 584),\n",
       " ('pull', 580),\n",
       " ('human', 578),\n",
       " ('expect', 577),\n",
       " ('goin', 576),\n",
       " ('touch', 576),\n",
       " ('rather', 574),\n",
       " ('except', 573),\n",
       " ('calling', 571),\n",
       " ('parents', 569),\n",
       " ('special', 569),\n",
       " ('known', 567),\n",
       " ('worked', 567),\n",
       " ('hair', 564),\n",
       " ('miles', 564),\n",
       " ('air', 563),\n",
       " ('act', 563),\n",
       " ('learn', 561),\n",
       " ('clear', 560),\n",
       " ('state', 558),\n",
       " ('none', 556),\n",
       " ('hospital', 552),\n",
       " ('kidding', 548),\n",
       " ('questions', 546),\n",
       " ('order', 544),\n",
       " ('works', 544),\n",
       " ('dream', 543),\n",
       " ('playing', 541),\n",
       " ('luck', 541),\n",
       " ('control', 540),\n",
       " ('takes', 538),\n",
       " ('catch', 537),\n",
       " ('mouth', 536),\n",
       " ('hotel', 535),\n",
       " ('mister', 532),\n",
       " ('mary', 532),\n",
       " ('secret', 530),\n",
       " ('explain', 527),\n",
       " ('buddy', 527),\n",
       " ('seven', 526),\n",
       " ('felt', 526),\n",
       " ('general', 526),\n",
       " ('follow', 525),\n",
       " ('perfect', 524),\n",
       " ('accident', 523),\n",
       " ('interested', 523),\n",
       " ('worse', 522),\n",
       " ('ride', 522),\n",
       " ('besides', 520),\n",
       " ('unless', 518),\n",
       " ('coffee', 517),\n",
       " ('bitch', 516),\n",
       " ('past', 513),\n",
       " ('mr', 513),\n",
       " ('simple', 512),\n",
       " ('joe', 511),\n",
       " ('earth', 509),\n",
       " ('absolutely', 508),\n",
       " ('date', 507),\n",
       " ('sam', 506),\n",
       " ('others', 506),\n",
       " ('talked', 502),\n",
       " ('seeing', 501),\n",
       " ('apartment', 501),\n",
       " ('loved', 500),\n",
       " ('certainly', 499),\n",
       " ('dark', 497),\n",
       " (\"c'mon\", 495),\n",
       " ('outta', 492),\n",
       " ('top', 492),\n",
       " ('wonder', 491),\n",
       " ('meeting', 490),\n",
       " ('throw', 487),\n",
       " ('turned', 487),\n",
       " ('report', 486),\n",
       " ('red', 486),\n",
       " ('tom', 485),\n",
       " ('drop', 483),\n",
       " ('hang', 483),\n",
       " ('mad', 481),\n",
       " ('music', 481),\n",
       " ('clean', 479),\n",
       " ('charlie', 477),\n",
       " ('swear', 475),\n",
       " ('month', 475),\n",
       " ('giving', 473),\n",
       " ('doin', 473),\n",
       " ('information', 472),\n",
       " ('army', 472),\n",
       " ('become', 472),\n",
       " ('cops', 471),\n",
       " ('less', 470),\n",
       " ('[', 469),\n",
       " ('smart', 466),\n",
       " ('sell', 465),\n",
       " ('fall', 463),\n",
       " ('paid', 463),\n",
       " ('worried', 463),\n",
       " ('choice', 463),\n",
       " ('bank', 463),\n",
       " ('fault', 463),\n",
       " ('handle', 461),\n",
       " ('major', 460),\n",
       " ('nine', 459),\n",
       " ('feet', 459),\n",
       " ('yesterday', 458),\n",
       " ('quit', 457),\n",
       " ('meant', 456),\n",
       " ('paul', 456),\n",
       " ('plane', 456),\n",
       " ('ones', 454),\n",
       " ('nothin', 451),\n",
       " ('caught', 451),\n",
       " ('kept', 450),\n",
       " ('win', 448),\n",
       " ('david', 448),\n",
       " ('boss', 448),\n",
       " ('terrible', 448),\n",
       " ('changed', 447),\n",
       " ('sweet', 447),\n",
       " ('cop', 446),\n",
       " ('thirty', 446),\n",
       " ('finish', 445),\n",
       " ('marry', 445),\n",
       " ('bob', 445),\n",
       " ('road', 445),\n",
       " ('blow', 444),\n",
       " ('strange', 443),\n",
       " ('paper', 443),\n",
       " ('figured', 442),\n",
       " ('sign', 442),\n",
       " ('difference', 439),\n",
       " ('record', 438),\n",
       " ('clothes', 438),\n",
       " ('murder', 438),\n",
       " ('given', 436),\n",
       " ('eh', 436),\n",
       " ('bucks', 436),\n",
       " ('law', 435),\n",
       " ('wear', 434),\n",
       " ('fucked', 433),\n",
       " ('personal', 433),\n",
       " ('walter', 432),\n",
       " ('certain', 432),\n",
       " ('mama', 432),\n",
       " ('anywhere', 430),\n",
       " ('class', 429),\n",
       " ('longer', 429),\n",
       " ('lord', 429),\n",
       " ('tv', 427),\n",
       " ('busy', 426),\n",
       " ('mistake', 426),\n",
       " ('watching', 426),\n",
       " ('uh-huh', 425),\n",
       " ('near', 425),\n",
       " ('store', 424),\n",
       " ('system', 421),\n",
       " ('early', 421),\n",
       " ('rich', 420),\n",
       " ('imagine', 420),\n",
       " ('private', 420),\n",
       " ('blue', 419),\n",
       " ('fair', 419),\n",
       " ('happening', 418),\n",
       " ('american', 417),\n",
       " ('liked', 417),\n",
       " ('kinda', 416),\n",
       " ('broke', 416),\n",
       " ('favor', 416),\n",
       " ('lying', 415),\n",
       " ('moving', 415),\n",
       " ('future', 415),\n",
       " ('fifteen', 414),\n",
       " ('building', 413),\n",
       " ('eye', 412),\n",
       " ('middle', 412),\n",
       " ('honest', 411),\n",
       " ('situation', 410),\n",
       " ('lunch', 409),\n",
       " ('boat', 407),\n",
       " ('careful', 406),\n",
       " ('jim', 405),\n",
       " ('trip', 405),\n",
       " ('hurry', 404),\n",
       " ('killing', 404),\n",
       " ('problems', 404),\n",
       " ('fool', 404),\n",
       " ('quiet', 403),\n",
       " ('sitting', 402),\n",
       " ('goodbye', 402),\n",
       " ('..', 401),\n",
       " ('respect', 400),\n",
       " ('michael', 400),\n",
       " ('involved', 398),\n",
       " ('rose', 398),\n",
       " ('sake', 398),\n",
       " ('completely', 397),\n",
       " ('ray', 397),\n",
       " ('calls', 396),\n",
       " ('tough', 394),\n",
       " ('uncle', 393),\n",
       " ('born', 392),\n",
       " ('instead', 392),\n",
       " ('agent', 392),\n",
       " ('dick', 391),\n",
       " ('floor', 391),\n",
       " ('honor', 391),\n",
       " ('twelve', 391),\n",
       " ('ought', 389),\n",
       " ('history', 388),\n",
       " ('realize', 387),\n",
       " ('kiss', 387),\n",
       " ('books', 387),\n",
       " ('ben', 386),\n",
       " ('interesting', 386),\n",
       " ('weird', 385),\n",
       " ('finally', 384),\n",
       " ('chief', 383),\n",
       " ('peter', 382),\n",
       " ('jimmy', 382),\n",
       " ('dangerous', 382),\n",
       " ('likes', 381),\n",
       " ('darling', 380),\n",
       " ('security', 380),\n",
       " ('officer', 380),\n",
       " ('drunk', 380),\n",
       " ('evening', 380),\n",
       " ('stick', 379),\n",
       " ('spend', 378),\n",
       " ('cash', 377),\n",
       " ('dude', 377),\n",
       " ('message', 376),\n",
       " ('brain', 376),\n",
       " ('needed', 376),\n",
       " ('land', 376),\n",
       " ('killer', 375),\n",
       " ('jail', 374),\n",
       " ('offer', 373),\n",
       " ('bought', 373),\n",
       " ('james', 373),\n",
       " ('nervous', 372),\n",
       " ('jake', 370),\n",
       " ('pain', 370),\n",
       " ('english', 370),\n",
       " ('window', 369),\n",
       " ('grand', 369),\n",
       " ('fear', 369),\n",
       " ('asshole', 368),\n",
       " ('dance', 368),\n",
       " ('nick', 366),\n",
       " ('stopped', 365),\n",
       " ('afternoon', 363),\n",
       " ('upset', 363),\n",
       " ('court', 363),\n",
       " ('ran', 363),\n",
       " ('somethin', 362),\n",
       " ('letter', 362),\n",
       " ('johnny', 362),\n",
       " ('lived', 361),\n",
       " ('radio', 361),\n",
       " ('bag', 361),\n",
       " ('key', 360),\n",
       " ('picked', 360),\n",
       " ('list', 360),\n",
       " ('service', 360),\n",
       " (\"o'clock\", 359),\n",
       " ('um', 359),\n",
       " ('finished', 359),\n",
       " ('relax', 359),\n",
       " ('decided', 358),\n",
       " ('doubt', 356),\n",
       " ('owe', 356),\n",
       " ('hardly', 356),\n",
       " ('max', 355),\n",
       " ('pictures', 354),\n",
       " ('wake', 354),\n",
       " ('count', 354),\n",
       " ('team', 354),\n",
       " ('tape', 353),\n",
       " ('lawyer', 353),\n",
       " ('college', 353),\n",
       " ('named', 352),\n",
       " ('table', 352),\n",
       " ('wrote', 351),\n",
       " ('ha', 350),\n",
       " ('totally', 350),\n",
       " ('neither', 350),\n",
       " ('across', 349),\n",
       " ('bother', 349),\n",
       " ('fix', 348),\n",
       " ('&', 348),\n",
       " ('bastard', 347),\n",
       " ('voice', 346),\n",
       " ('age', 344),\n",
       " ('carry', 344),\n",
       " ('pass', 344),\n",
       " ('prove', 342),\n",
       " ('ice', 341),\n",
       " ('eddie', 340),\n",
       " ('machine', 339),\n",
       " (\"ma'am\", 339),\n",
       " ('staying', 338),\n",
       " ('ring', 338),\n",
       " ('third', 338),\n",
       " ('bunch', 338),\n",
       " ('return', 336),\n",
       " ('forgive', 335),\n",
       " ('public', 335),\n",
       " ('evil', 335),\n",
       " ('driving', 335),\n",
       " ('welcome', 334),\n",
       " ('christmas', 334),\n",
       " ('strong', 334),\n",
       " ('hungry', 332),\n",
       " ('appreciate', 332),\n",
       " ('gotten', 331),\n",
       " ('listening', 330),\n",
       " ('forgot', 330),\n",
       " ('crime', 329),\n",
       " ('deep', 329),\n",
       " ('putting', 329),\n",
       " ('government', 328),\n",
       " ('attack', 328),\n",
       " ('standing', 327),\n",
       " ('cost', 327),\n",
       " ('prison', 326),\n",
       " ('force', 326),\n",
       " ('quick', 326),\n",
       " ('church', 326),\n",
       " ('lots', 326),\n",
       " ('missed', 326),\n",
       " ('short', 324),\n",
       " (\"'bout\", 324),\n",
       " ('bar', 324),\n",
       " ('step', 324),\n",
       " ('fly', 324),\n",
       " ('wow', 323),\n",
       " ('dying', 322),\n",
       " ('art', 322),\n",
       " ('pleasure', 322),\n",
       " ('moved', 322),\n",
       " ('dress', 321),\n",
       " ('space', 320),\n",
       " ('french', 319),\n",
       " ('ma', 319),\n",
       " ('charge', 318),\n",
       " ('suit', 317),\n",
       " ('usually', 317),\n",
       " ('island', 317),\n",
       " ('saved', 316),\n",
       " ('missing', 316),\n",
       " ('calm', 315),\n",
       " ('slow', 314),\n",
       " ('joke', 314),\n",
       " ('entire', 314),\n",
       " ('club', 314),\n",
       " ('mark', 313),\n",
       " ('train', 313),\n",
       " ('papers', 313),\n",
       " ('gold', 312),\n",
       " ('fish', 312),\n",
       " ('truck', 312),\n",
       " ('station', 312),\n",
       " ('board', 312),\n",
       " ('grow', 312),\n",
       " ('movies', 312),\n",
       " ('present', 311),\n",
       " ('department', 311),\n",
       " ('lead', 311),\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_occ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords, \"i\", \"you\" are too frequent. Should be removed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(tokenized_questions, tokenized_answers, special_symbols):\n",
    "    word2id = {}\n",
    "    id2word = []\n",
    "    \n",
    "    for special_symbol in special_symbols:\n",
    "        id2word.append(special_symbol)\n",
    "        word2id[special_symbol] = id2word.index(special_symbol)\n",
    "        \n",
    "    vocab_set = set(word for dataset in [tokenized_questions, tokenized_answers]\n",
    "                    for sentence in dataset\n",
    "                    for word in sentence\n",
    "                    if word not in special_symbols)\n",
    "     \n",
    "    for word in vocab_set:\n",
    "        id2word.append(word)\n",
    "        word2id[word] = id2word.index(word)\n",
    "        \n",
    "    return word2id, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id, id2word = build_dict(tokenized_questions_train, tokenized_answers_train, special_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_unk(dataset, word2id):\n",
    "    replaced_dataset = []\n",
    "    for sentence in dataset:\n",
    "        for i, word in enumerate(sentence):\n",
    "            if word not in word2id.keys():\n",
    "                sentence[i] = '<UNK>'\n",
    "        replaced_dataset.append(sentence)\n",
    "        \n",
    "    return replaced_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_questions_test = replace_with_unk(tokenized_questions_test, word2id)\n",
    "tokenized_answers_test = replace_with_unk(tokenized_answers_test, word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check oov\n",
    "oov = []\n",
    "for dataset in [tokenized_questions_test, tokenized_answers_test]:\n",
    "    for sentence in dataset:\n",
    "        for word in sentence:\n",
    "            if word not in word2id.keys():\n",
    "                oov.append(word)\n",
    "set(oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1946"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of <UNK> in test set\n",
    "count = 0\n",
    "for dataset in [tokenized_questions_test, tokenized_answers_test]:\n",
    "    for sentence in dataset:\n",
    "        for word in sentence:\n",
    "            if word == '<UNK>':\n",
    "                count += 1\n",
    "                \n",
    "count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embeddings(word2id, embeddings, dim=300):\n",
    "    vocab_size = len(word2id)\n",
    "    embedding_matrix = np.random.normal(0, 1, (vocab_size, dim))\n",
    "    \n",
    "    for word, i in word2id.items():\n",
    "        try:\n",
    "            embedding_vector = embeddings.get_vector(word)\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "customized_embeddings = build_embeddings(word2id, embeddings, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save customized_embeddings\n",
    "path = 'word_embeddings.txt'\n",
    "\n",
    "np.savetxt(path, customized_embeddings, delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save word2id dictionary\n",
    "path = 'word2id.txt'\n",
    "\n",
    "out = open(path, 'w')\n",
    "for word, i in word2id.items():\n",
    "    print(word, i, sep=' ', file=out)\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_ids(tokenized_sentence, word2id, padded_len):\n",
    "    num_pad = max(0, padded_len - 1 - len(tokenized_sentence))\n",
    "    sent = tokenized_sentence[:padded_len-1] + ['</S>']\n",
    "    sent = sent + ['<PAD>']*num_pad\n",
    "    sent_ids = [word2id[word] for word in sent]\n",
    "    \n",
    "    sent_len = min(len(tokenized_sentence)+1, padded_len)\n",
    "    \n",
    "    return sent_ids, sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_sentence(ids, id2word):\n",
    "    return [id2word[i] for i in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_ids(sentences, word2id, max_len):\n",
    "    max_len_in_batch = min(max(len(s) for s in sentences) + 1, max_len)\n",
    "    batch_ids, batch_ids_len = [], []\n",
    "    for sentence in sentences:\n",
    "        ids, ids_len = sentence_to_ids(sentence, word2id, max_len_in_batch)\n",
    "        batch_ids.append(ids)\n",
    "        batch_ids_len.append(ids_len)\n",
    "        \n",
    "    return batch_ids, batch_ids_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(samples, batch_size=32):\n",
    "    X, Y = [], []\n",
    "    for i, (x, y) in enumerate(samples, 1):\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "        if i % batch_size == 0:\n",
    "            yield X, Y\n",
    "            X, Y = [], []\n",
    "    if X and Y:\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: (['he', \"'s\", 'in', 'jamaica', 'with', 'a', 'twenty-three-', 'year-old', '.', 'a', 'friend', 'of', 'my', 'daughter', \"'s\", '.', 'he', 'had', 'the', 'fucking', 'nerve', 'to', 'call', 'me', 'and', 'ask', 'me', 'to', 'borrow', 'some', 'money', 'and', 'i', 'told', 'him', 'to', 'fuck', 'off', ',', 'so', 'he', 'asked', 'me', 'to', 'sell', 'his', 'singles', 'collection', 'and', 'send', 'him', 'a', 'check', 'for', 'whatever', 'i', 'go', ',', 'minus', 'a', 'ten', 'percent', 'commission', '.', 'which', 'reminds', 'me', '.', 'can', 'you', 'make', 'sure', 'you', 'give', 'me', 'a', 'five', '?', 'i', 'want', 'to', 'frame', 'it', 'and', 'put', 'it', 'on', 'the', 'wall', '.'], ['it', 'must', 'have', 'taken', 'him', 'a', 'long', 'time', 'to', 'get', 'them', 'together', '.'])\n",
      "Ids: [[19918, 52248, 48184, 28312, 21456, 32223, 17323, 26616, 45234, 1], [23978, 19668, 19627, 15925, 26524, 32223, 48496, 28852, 29618, 1]]\n",
      "Sentences lengths: [10, 10]\n"
     ]
    }
   ],
   "source": [
    "sentences = list(zip(tokenized_questions_train, tokenized_answers_train))[0]\n",
    "ids, sent_lens = batch_to_ids(sentences, word2id, max_len=10)\n",
    "\n",
    "print('Input:', sentences)\n",
    "print('Ids: {}\\nSentences lengths: {}'.format(ids, sent_lens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel(object):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to create [placeholders](https://www.tensorflow.org/api_guides/python/io_ops#Placeholders) to specify what data we are going to feed into the network during the execution time. For this task we will need:\n",
    " - *input_batch* — sequences of sentences (the shape will equal to [batch_size, max_sequence_len_in_batch]);\n",
    " - *input_batch_lengths* — lengths of not padded sequences (the shape equals to [batch_size]);\n",
    " - *ground_truth* — sequences of groundtruth (the shape will equal to [batch_size, max_sequence_len_in_batch]);\n",
    " - *ground_truth_lengths* — lengths of not padded groundtruth sequences (the shape equals to [batch_size]);\n",
    " - *dropout_ph* — dropout keep probability; this placeholder has a predifined value 1;\n",
    " - *learning_rate_ph* — learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def declare_placeholders(self):\n",
    "    # placeholders for input and its actual lengths\n",
    "    self.input_batch = tf.placeholder(shape=(None, None), dtype=tf.int32, name='input_batch')\n",
    "    self.input_batch_lengths = tf.placeholder(shape=(None, ), dtype=tf.int32, name='input_batch_lengths')\n",
    "    \n",
    "    # placeholders for groundtruth and its actual lenghts\n",
    "    self.ground_truth = tf.placeholder(shape=(None, None), dtype=tf.int32, name='ground_truth')\n",
    "    self.ground_truth_lengths = tf.placeholder(shape=(None, ), dtype=tf.int32, name='ground_truth_lengths')\n",
    "    \n",
    "    # placeholders for dropout_rate and learning_rate\n",
    "    self.dropout_ph = tf.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[])\n",
    "    self.learning_rate_ph = tf.placeholder(dtype=tf.float32, shape=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq2SeqModel.__declare_placeholders = classmethod(declare_placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(self, embeddings_matrix):\n",
    "    self.embeddings = tf.get_variable(name='embeddings', \n",
    "                                     shape=embeddings_matrix.shape,\n",
    "                                     initializer=tf.constant_initializer(embeddings_matrix),\n",
    "                                     trainable=False)\n",
    "    self.input_batch_embedded = tf.nn.embedding_lookup(self.embeddings, self.input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq2SeqModel.__create_embeddings = classmethod(create_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder(self, hidden_size):\n",
    "    forward_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "        tf.nn.rnn_cell.BasicLSTMCell(num_units=hidden_size),\n",
    "        input_keep_prob=self.dropout_ph,\n",
    "        output_keep_prob=self.dropout_ph,\n",
    "        state_keep_prob=self.dropout_ph)\n",
    "    \n",
    "    backward_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "        tf.nn.rnn_cell.BasicLSTMCell(num_units=hidden_size),\n",
    "        input_keep_prob=self.dropout_ph,\n",
    "        output_keep_prob=self.dropout_ph,\n",
    "        state_keep_prob=self.dropout_ph)\n",
    "    \n",
    "    output, final_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_fw=forward_cell,\n",
    "        cell_bw=backward_cell,\n",
    "        inputs=self.input_batch_embedded,\n",
    "        sequence_length=self.input_batch_lengths,\n",
    "        dtype=tf.float32)\n",
    "    \n",
    "    self.encoder_output = tf.concat([output[0], output[1]], axis=2)\n",
    "    \n",
    "    encoder_final_state_c = tf.concat([final_state[0].c, final_state[1].c], axis=1)\n",
    "    encoder_final_state_h = tf.concat([final_state[0].h, final_state[1].h], axis=1)\n",
    "    self.encoder_final_state = tf.contrib.rnn.LSTMStateTuple(c=encoder_final_state_c, h=encoder_final_state_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq2SeqModel.__build_encoder = classmethod(build_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decoder(self, hidden_size, vocab_size, max_iter, start_symbol_id, end_symbol_id):\n",
    "    batch_size = tf.shape(self.input_batch)[0]\n",
    "    start_tokens = tf.fill([batch_size], start_symbol_id)\n",
    "    ground_truth_as_input = tf.concat([tf.expand_dims(start_tokens, 1), self.ground_truth], 1)\n",
    "    \n",
    "    # Use the embedding layer defined before to lookup embedings for ground_truth_as_input\n",
    "    self.ground_truth_embedded = tf.nn.embedding_lookup(self.embeddings, ground_truth_as_input)\n",
    "    \n",
    "    # Create TrainingHelper for the train stage\n",
    "    train_helper = tf.contrib.seq2seq.TrainingHelper(self.ground_truth_embedded,\n",
    "                                                     self.ground_truth_lengths)\n",
    "        \n",
    "    # Create GreedyEmbeddingHelper for the inference stage\n",
    "    infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(self.embeddings, start_tokens, end_symbol_id)\n",
    "    \n",
    "    def decode(helper, scope, reuse=None):\n",
    "        \"\"\"Creates decoder and return the results of the decoding with a given helper.\"\"\"\n",
    "        \n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                num_units=hidden_size, \n",
    "                memory=self.encoder_output,\n",
    "                memory_sequence_length=self.input_batch_lengths)\n",
    "            \n",
    "            cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=hidden_size*2, reuse=reuse)\n",
    "#             cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.BasicLSTMCell(num_units=hidden_size, reuse=reuse),\n",
    "#                                                 input_keep_prob=self.dropout_ph,\n",
    "#                                                 output_keep_prob=self.dropout_ph,\n",
    "#                                                 state_keep_prob=self.dropout_ph)\n",
    "    \n",
    "            attention_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                cell, attention_mechanism, attention_layer_size=hidden_size)\n",
    "#             decoder_cell = tf.contrib.rnn.DropoutWrapper(tf.nn.rnn_cell.BasicLSTMCell(num_units=hidden_size, reuse=reuse))\n",
    "            \n",
    "            decoder_cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "                attention_cell, vocab_size, reuse=reuse)\n",
    "            \n",
    "            decoder_initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=batch_size)\n",
    "            decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                cell=decoder_cell,\n",
    "                helper=helper,\n",
    "                initial_state=decoder_initial_state)\n",
    "            \n",
    "            outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                decoder=decoder,\n",
    "                maximum_iterations=max_iter,\n",
    "                output_time_major=False,\n",
    "                impute_finished=True)\n",
    "            \n",
    "            return outputs\n",
    "    \n",
    "    self.train_outputs = decode(train_helper, 'decode')\n",
    "    self.infer_outputs = decode(infer_helper, 'decode', reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq2SeqModel.__build_decoder = classmethod(build_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(self):\n",
    "    \"\"\"Computes sequence loss (masked cross-entopy loss with logits).\"\"\"\n",
    "    \n",
    "    weights = tf.cast(tf.sequence_mask(self.ground_truth_lengths), dtype=tf.float32)\n",
    "    \n",
    "    self.loss = tf.contrib.seq2seq.sequence_loss(self.train_outputs.rnn_output,\n",
    "                                                 self.ground_truth,\n",
    "                                                 weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq2SeqModel.__compute_loss = classmethod(compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_optimization(self):\n",
    "    self.train_op = tf.contrib.layers.optimize_loss(loss=self.loss,\n",
    "                                                    optimizer='Adam',\n",
    "                                                    learning_rate=self.learning_rate_ph,\n",
    "                                                    clip_gradients=1.0,\n",
    "                                                    global_step=tf.train.get_global_step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq2SeqModel.__perform_optimization = classmethod(perform_optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(self, embeddings_matrix, hidden_size, vocab_size, max_iter, \n",
    "               start_symbol_id, end_symbol_id, padding_symbol_id):\n",
    "    self.__declare_placeholders()\n",
    "    self.__create_embeddings(embeddings_matrix)\n",
    "    self.__build_encoder(hidden_size)\n",
    "    self.__build_decoder(hidden_size, vocab_size, max_iter, start_symbol_id, end_symbol_id)\n",
    "    \n",
    "    self.__compute_loss()\n",
    "    self.__perform_optimization()\n",
    "    \n",
    "    self.train_predictions = self.train_outputs.sample_id\n",
    "    self.infer_predictions = self.infer_outputs.sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq2SeqModel.__init__ = classmethod(init_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network and predict output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(self, session, X, X_seq_len, Y, Y_seq_len, learning_rate, dropout_keep_probability):\n",
    "    feed_dict = {\n",
    "            self.input_batch: X,\n",
    "            self.input_batch_lengths: X_seq_len,\n",
    "            self.ground_truth: Y,\n",
    "            self.ground_truth_lengths: Y_seq_len,\n",
    "            self.learning_rate_ph: learning_rate,\n",
    "            self.dropout_ph: dropout_keep_probability\n",
    "        }\n",
    "    pred, loss, _ = session.run([\n",
    "            self.train_predictions,\n",
    "            self.loss,\n",
    "            self.train_op], feed_dict=feed_dict)\n",
    "    return pred, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq2SeqModel.train_on_batch = classmethod(train_on_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_batch(self, session, X, X_seq_len):\n",
    "    feed_dict = {self.input_batch: X, self.input_batch_lengths: X_seq_len}\n",
    "    pred = session.run([\n",
    "            self.infer_predictions\n",
    "        ], feed_dict=feed_dict)[0]\n",
    "    return pred\n",
    "\n",
    "def predict_for_batch_with_loss(self, session, X, X_seq_len, Y, Y_seq_len):\n",
    "    feed_dict = {self.input_batch: X, \n",
    "                 self.input_batch_lengths: X_seq_len,\n",
    "                 self.ground_truth: Y,\n",
    "                 self.ground_truth_lengths: Y_seq_len}\n",
    "    pred, loss = session.run([\n",
    "            self.infer_predictions,\n",
    "            self.loss,\n",
    "        ], feed_dict=feed_dict)\n",
    "    return pred, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq2SeqModel.predict_for_batch = classmethod(predict_for_batch)\n",
    "Seq2SeqModel.predict_for_batch_with_loss = classmethod(predict_for_batch_with_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = Seq2SeqModel(\n",
    "    embeddings_matrix=customized_embeddings,\n",
    "    hidden_size=128,\n",
    "    vocab_size=customized_embeddings.shape[0],\n",
    "    max_iter=15, \n",
    "    start_symbol_id=word2id['<S>'],\n",
    "    end_symbol_id=word2id['</S>'],\n",
    "    padding_symbol_id=word2id['<PAD>'])\n",
    "\n",
    "batch_size = 32\n",
    "n_epochs = 10\n",
    "learning_rate = 0.001\n",
    "dropout_keep_probability = 0.5\n",
    "max_len = 15\n",
    "learning_rate_decay = 0.75\n",
    "min_learning_rate = 0.0001\n",
    "\n",
    "n_step = int(len(questions_train)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train: epoch 1\n",
      "Epoch:   1/10, Step:    1/6232, Loss: 10.960, Seconds: 462.76\n",
      "\n",
      "Epoch:   1/10, Step:  201/6232, Loss:  5.357, Seconds: 62.03\n",
      "\n",
      "Epoch:   1/10, Step:  401/6232, Loss:  4.995, Seconds: 61.63\n",
      "\n",
      "Epoch:   1/10, Step:  601/6232, Loss:  4.421, Seconds: 62.23\n",
      "\n",
      "Epoch:   1/10, Step:  801/6232, Loss:  4.598, Seconds: 62.03\n",
      "\n",
      "Epoch:   1/10, Step: 1001/6232, Loss:  4.427, Seconds: 61.64\n",
      "\n",
      "Epoch:   1/10, Step: 1201/6232, Loss:  4.612, Seconds: 61.83\n",
      "\n",
      "Epoch:   1/10, Step: 1401/6232, Loss:  4.435, Seconds: 61.83\n",
      "\n",
      "Epoch:   1/10, Step: 1601/6232, Loss:  4.366, Seconds: 61.83\n",
      "\n",
      "Epoch:   1/10, Step: 1801/6232, Loss:  4.608, Seconds: 61.84\n",
      "\n",
      "Epoch:   1/10, Step: 2001/6232, Loss:  4.251, Seconds: 61.44\n",
      "\n",
      "Epoch:   1/10, Step: 2201/6232, Loss:  4.604, Seconds: 61.63\n",
      "\n",
      "Epoch:   1/10, Step: 2401/6232, Loss:  4.492, Seconds: 61.84\n",
      "\n",
      "Epoch:   1/10, Step: 2601/6232, Loss:  4.551, Seconds: 61.83\n",
      "\n",
      "Epoch:   1/10, Step: 2801/6232, Loss:  4.552, Seconds: 61.83\n",
      "\n",
      "Epoch:   1/10, Step: 3001/6232, Loss:  4.407, Seconds: 61.63\n",
      "\n",
      "Epoch:   1/10, Step: 3201/6232, Loss:  4.229, Seconds: 61.64\n",
      "\n",
      "Epoch:   1/10, Step: 3401/6232, Loss:  4.219, Seconds: 62.23\n",
      "\n",
      "Epoch:   1/10, Step: 3601/6232, Loss:  4.785, Seconds: 61.83\n",
      "\n",
      "Epoch:   1/10, Step: 3801/6232, Loss:  4.795, Seconds: 61.83\n",
      "\n",
      "Epoch:   1/10, Step: 4001/6232, Loss:  4.268, Seconds: 61.44\n",
      "\n",
      "Epoch:   1/10, Step: 4201/6232, Loss:  4.073, Seconds: 61.44\n",
      "\n",
      "Epoch:   1/10, Step: 4401/6232, Loss:  4.218, Seconds: 61.64\n",
      "\n",
      "Epoch:   1/10, Step: 4601/6232, Loss:  4.079, Seconds: 62.03\n",
      "\n",
      "Epoch:   1/10, Step: 4801/6232, Loss:  4.568, Seconds: 61.64\n",
      "\n",
      "Epoch:   1/10, Step: 5001/6232, Loss:  4.464, Seconds: 61.83\n",
      "\n",
      "Epoch:   1/10, Step: 5201/6232, Loss:  4.293, Seconds: 61.64\n",
      "\n",
      "Epoch:   1/10, Step: 5401/6232, Loss:  4.172, Seconds: 62.03\n",
      "\n",
      "Epoch:   1/10, Step: 5601/6232, Loss:  4.248, Seconds: 61.84\n",
      "\n",
      "Epoch:   1/10, Step: 5801/6232, Loss:  4.962, Seconds: 62.03\n",
      "\n",
      "Epoch:   1/10, Step: 6001/6232, Loss:  4.114, Seconds: 62.23\n",
      "\n",
      "Epoch:   1/10, Step: 6201/6232, Loss:  4.329, Seconds: 61.64\n",
      "\n",
      "Test: epoch 1 loss 4.2718663 Second: 153.70115685462952\n",
      "X: according to my calculations , we should be right under the statue . we </S>\n",
      "Y: and when those dirty yanks go to sleep -- no offense . </S> <PAD> <PAD>\n",
      "O: i 'm not going to get a lot of a man . </S>\n",
      "\n",
      "X: mine . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Y: have you always had it ? </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "O: i 'm not . </S> <S> <S> <S> <S> <S> <S> <S> <S>\n",
      "\n",
      "X: well , as i 've said , we 've heard about you . we </S>\n",
      "Y: thank you very much , sir . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "O: i 'm not . </S> <S> <S> <S> <S> <S> <S> <S> <S>\n",
      "\n",
      "No Improvement\n",
      "------------------------------\n",
      "Train: epoch 2\n",
      "Epoch:   2/10, Step:    1/6232, Loss:  3.810, Seconds: 64.62\n",
      "\n",
      "Epoch:   2/10, Step:  201/6232, Loss:  4.024, Seconds: 61.84\n",
      "\n",
      "Epoch:   2/10, Step:  401/6232, Loss:  4.061, Seconds: 61.83\n",
      "\n",
      "Epoch:   2/10, Step:  601/6232, Loss:  4.156, Seconds: 61.83\n",
      "\n",
      "Epoch:   2/10, Step:  801/6232, Loss:  4.121, Seconds: 62.03\n",
      "\n",
      "Epoch:   2/10, Step: 1001/6232, Loss:  4.120, Seconds: 62.24\n",
      "\n",
      "Epoch:   2/10, Step: 1201/6232, Loss:  4.156, Seconds: 62.03\n",
      "\n",
      "Epoch:   2/10, Step: 1401/6232, Loss:  3.834, Seconds: 62.24\n",
      "\n",
      "Epoch:   2/10, Step: 1601/6232, Loss:  3.934, Seconds: 61.83\n",
      "\n",
      "Epoch:   2/10, Step: 1801/6232, Loss:  4.313, Seconds: 62.04\n",
      "\n",
      "Epoch:   2/10, Step: 2001/6232, Loss:  4.137, Seconds: 62.03\n",
      "\n",
      "Epoch:   2/10, Step: 2201/6232, Loss:  4.219, Seconds: 61.83\n",
      "\n",
      "Epoch:   2/10, Step: 2401/6232, Loss:  3.617, Seconds: 61.83\n",
      "\n",
      "Epoch:   2/10, Step: 2601/6232, Loss:  4.174, Seconds: 62.03\n",
      "\n",
      "Epoch:   2/10, Step: 2801/6232, Loss:  4.227, Seconds: 61.83\n",
      "\n",
      "Epoch:   2/10, Step: 3001/6232, Loss:  4.028, Seconds: 62.43\n",
      "\n",
      "Epoch:   2/10, Step: 3201/6232, Loss:  3.896, Seconds: 61.44\n",
      "\n",
      "Epoch:   2/10, Step: 3401/6232, Loss:  4.487, Seconds: 62.03\n",
      "\n",
      "Epoch:   2/10, Step: 3601/6232, Loss:  4.111, Seconds: 62.43\n",
      "\n",
      "Epoch:   2/10, Step: 3801/6232, Loss:  4.171, Seconds: 62.03\n",
      "\n",
      "Epoch:   2/10, Step: 4001/6232, Loss:  3.885, Seconds: 62.23\n",
      "\n",
      "Epoch:   2/10, Step: 4201/6232, Loss:  4.222, Seconds: 61.84\n",
      "\n",
      "Epoch:   2/10, Step: 4401/6232, Loss:  3.713, Seconds: 62.23\n",
      "\n",
      "Epoch:   2/10, Step: 4601/6232, Loss:  4.017, Seconds: 62.03\n",
      "\n",
      "Epoch:   2/10, Step: 4801/6232, Loss:  4.080, Seconds: 62.43\n",
      "\n",
      "Epoch:   2/10, Step: 5001/6232, Loss:  4.072, Seconds: 61.64\n",
      "\n",
      "Epoch:   2/10, Step: 5201/6232, Loss:  4.091, Seconds: 62.63\n",
      "\n",
      "Epoch:   2/10, Step: 5401/6232, Loss:  3.652, Seconds: 62.83\n",
      "\n",
      "Epoch:   2/10, Step: 5601/6232, Loss:  3.831, Seconds: 61.64\n",
      "\n",
      "Epoch:   2/10, Step: 5801/6232, Loss:  3.951, Seconds: 62.23\n",
      "\n",
      "Epoch:   2/10, Step: 6001/6232, Loss:  4.250, Seconds: 62.63\n",
      "\n",
      "Epoch:   2/10, Step: 6201/6232, Loss:  4.123, Seconds: 62.03\n",
      "\n",
      "Test: epoch 2 loss 4.1441503 Second: 131.04963731765747\n",
      "X: according to my calculations , we should be right under the statue . we </S>\n",
      "Y: and when those dirty yanks go to sleep -- no offense . </S> <PAD> <PAD>\n",
      "O: i do n't know . </S> <S> <S> <S> <S>\n",
      "\n",
      "X: mine . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Y: have you always had it ? </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "O: i 'm not going to be a little . </S>\n",
      "\n",
      "X: well , as i 've said , we 've heard about you . we </S>\n",
      "Y: thank you very much , sir . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "O: i 'm not sure . </S> <S> <S> <S> <S>\n",
      "\n",
      "No Improvement\n",
      "------------------------------\n",
      "Train: epoch 3\n",
      "Epoch:   3/10, Step:    1/6232, Loss:  3.932, Seconds: 64.22\n",
      "\n",
      "Epoch:   3/10, Step:  201/6232, Loss:  3.777, Seconds: 62.43\n",
      "\n",
      "Epoch:   3/10, Step:  401/6232, Loss:  3.967, Seconds: 62.23\n",
      "\n",
      "Epoch:   3/10, Step:  601/6232, Loss:  3.772, Seconds: 62.23\n",
      "\n",
      "Epoch:   3/10, Step:  801/6232, Loss:  4.206, Seconds: 61.64\n",
      "\n",
      "Epoch:   3/10, Step: 1001/6232, Loss:  4.063, Seconds: 61.83\n",
      "\n",
      "Epoch:   3/10, Step: 1201/6232, Loss:  4.159, Seconds: 61.64\n",
      "\n",
      "Epoch:   3/10, Step: 1401/6232, Loss:  3.995, Seconds: 61.44\n",
      "\n",
      "Epoch:   3/10, Step: 1601/6232, Loss:  4.329, Seconds: 61.63\n",
      "\n",
      "Epoch:   3/10, Step: 1801/6232, Loss:  3.801, Seconds: 62.03\n",
      "\n",
      "Epoch:   3/10, Step: 2001/6232, Loss:  3.745, Seconds: 61.83\n",
      "\n",
      "Epoch:   3/10, Step: 2201/6232, Loss:  3.670, Seconds: 62.23\n",
      "\n",
      "Epoch:   3/10, Step: 2401/6232, Loss:  4.252, Seconds: 61.63\n",
      "\n",
      "Epoch:   3/10, Step: 2601/6232, Loss:  4.074, Seconds: 62.04\n",
      "\n",
      "Epoch:   3/10, Step: 2801/6232, Loss:  3.972, Seconds: 61.83\n",
      "\n",
      "Epoch:   3/10, Step: 3001/6232, Loss:  3.718, Seconds: 62.23\n",
      "\n",
      "Epoch:   3/10, Step: 3201/6232, Loss:  4.044, Seconds: 62.04\n",
      "\n",
      "Epoch:   3/10, Step: 3401/6232, Loss:  4.266, Seconds: 62.24\n",
      "\n",
      "Epoch:   3/10, Step: 3601/6232, Loss:  4.048, Seconds: 61.44\n",
      "\n",
      "Epoch:   3/10, Step: 3801/6232, Loss:  3.863, Seconds: 62.23\n",
      "\n",
      "Epoch:   3/10, Step: 4001/6232, Loss:  3.916, Seconds: 62.43\n",
      "\n",
      "Epoch:   3/10, Step: 4201/6232, Loss:  4.108, Seconds: 61.64\n",
      "\n",
      "Epoch:   3/10, Step: 4401/6232, Loss:  4.038, Seconds: 63.63\n",
      "\n",
      "Epoch:   3/10, Step: 4601/6232, Loss:  4.211, Seconds: 61.83\n",
      "\n",
      "Epoch:   3/10, Step: 4801/6232, Loss:  4.108, Seconds: 62.63\n",
      "\n",
      "Epoch:   3/10, Step: 5001/6232, Loss:  4.325, Seconds: 62.34\n",
      "\n",
      "Epoch:   3/10, Step: 5201/6232, Loss:  3.761, Seconds: 62.24\n",
      "\n",
      "Epoch:   3/10, Step: 5401/6232, Loss:  4.365, Seconds: 62.46\n",
      "\n",
      "Epoch:   3/10, Step: 5601/6232, Loss:  4.020, Seconds: 62.23\n",
      "\n",
      "Epoch:   3/10, Step: 5801/6232, Loss:  3.919, Seconds: 63.23\n",
      "\n",
      "Epoch:   3/10, Step: 6001/6232, Loss:  3.599, Seconds: 62.64\n",
      "\n",
      "Epoch:   3/10, Step: 6201/6232, Loss:  3.639, Seconds: 62.43\n",
      "\n",
      "Test: epoch 3 loss 4.1428595 Second: 169.6413962841034\n",
      "X: according to my calculations , we should be right under the statue . we </S>\n",
      "Y: and when those dirty yanks go to sleep -- no offense . </S> <PAD> <PAD>\n",
      "O: i do n't know . </S> <S> <S> <S> <S> <S> <S> <S> <S> <S>\n",
      "\n",
      "X: mine . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Y: have you always had it ? </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "O: i 'm not going to see you . </S> <S> <S> <S> <S> <S> <S>\n",
      "\n",
      "X: well , as i 've said , we 've heard about you . we </S>\n",
      "Y: thank you very much , sir . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "O: i 'm not sure . </S> <S> <S> <S> <S> <S> <S> <S> <S> <S>\n",
      "\n",
      "New Record!\n",
      "\n",
      "------------------------------\n",
      "Train: epoch 4\n",
      "Epoch:   4/10, Step:    1/6232, Loss:  3.822, Seconds: 65.02\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   4/10, Step:  201/6232, Loss:  4.054, Seconds: 63.43\n",
      "\n",
      "Epoch:   4/10, Step:  401/6232, Loss:  3.967, Seconds: 61.81\n",
      "\n",
      "Epoch:   4/10, Step:  601/6232, Loss:  3.605, Seconds: 62.24\n",
      "\n",
      "Epoch:   4/10, Step:  801/6232, Loss:  3.959, Seconds: 62.74\n",
      "\n",
      "Epoch:   4/10, Step: 1001/6232, Loss:  3.713, Seconds: 62.71\n",
      "\n",
      "Epoch:   4/10, Step: 1201/6232, Loss:  3.906, Seconds: 62.19\n",
      "\n",
      "Epoch:   4/10, Step: 1401/6232, Loss:  3.877, Seconds: 62.37\n",
      "\n",
      "Epoch:   4/10, Step: 1601/6232, Loss:  3.759, Seconds: 61.84\n",
      "\n",
      "Epoch:   4/10, Step: 1801/6232, Loss:  3.970, Seconds: 62.43\n",
      "\n",
      "Epoch:   4/10, Step: 2001/6232, Loss:  4.039, Seconds: 61.96\n",
      "\n",
      "Epoch:   4/10, Step: 2201/6232, Loss:  3.613, Seconds: 62.20\n",
      "\n",
      "Epoch:   4/10, Step: 2401/6232, Loss:  3.777, Seconds: 62.03\n",
      "\n",
      "Epoch:   4/10, Step: 2601/6232, Loss:  3.690, Seconds: 63.03\n",
      "\n",
      "Epoch:   4/10, Step: 2801/6232, Loss:  3.738, Seconds: 62.14\n",
      "\n",
      "Epoch:   4/10, Step: 3001/6232, Loss:  3.760, Seconds: 62.21\n",
      "\n",
      "Epoch:   4/10, Step: 3201/6232, Loss:  4.226, Seconds: 61.83\n",
      "\n",
      "Epoch:   4/10, Step: 3401/6232, Loss:  4.024, Seconds: 62.69\n",
      "\n",
      "Epoch:   4/10, Step: 3601/6232, Loss:  3.964, Seconds: 61.84\n",
      "\n",
      "Epoch:   4/10, Step: 3801/6232, Loss:  3.619, Seconds: 62.43\n",
      "\n",
      "Epoch:   4/10, Step: 4001/6232, Loss:  3.779, Seconds: 62.04\n",
      "\n",
      "Epoch:   4/10, Step: 4201/6232, Loss:  3.624, Seconds: 62.03\n",
      "\n",
      "Epoch:   4/10, Step: 4401/6232, Loss:  3.785, Seconds: 61.83\n",
      "\n",
      "Epoch:   4/10, Step: 4601/6232, Loss:  4.086, Seconds: 62.23\n",
      "\n",
      "Epoch:   4/10, Step: 4801/6232, Loss:  3.684, Seconds: 62.43\n",
      "\n",
      "Epoch:   4/10, Step: 5001/6232, Loss:  3.613, Seconds: 62.08\n",
      "\n",
      "Epoch:   4/10, Step: 5201/6232, Loss:  3.683, Seconds: 62.63\n",
      "\n",
      "Epoch:   4/10, Step: 5401/6232, Loss:  3.587, Seconds: 62.63\n",
      "\n",
      "Epoch:   4/10, Step: 5601/6232, Loss:  3.835, Seconds: 62.83\n",
      "\n",
      "Epoch:   4/10, Step: 5801/6232, Loss:  3.503, Seconds: 62.59\n",
      "\n",
      "Epoch:   4/10, Step: 6001/6232, Loss:  4.043, Seconds: 62.23\n",
      "\n",
      "Epoch:   4/10, Step: 6201/6232, Loss:  3.638, Seconds: 62.24\n",
      "\n",
      "Test: epoch 4 loss 4.087086 Second: 139.5207712650299\n",
      "X: according to my calculations , we should be right under the statue . we </S>\n",
      "Y: and when those dirty yanks go to sleep -- no offense . </S> <PAD> <PAD>\n",
      "O: i do n't know . </S> <S> <S> <S> <S> <S>\n",
      "\n",
      "X: mine . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Y: have you always had it ? </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "O: i 'm sorry . </S> <S> <S> <S> <S> <S> <S>\n",
      "\n",
      "X: well , as i 've said , we 've heard about you . we </S>\n",
      "Y: thank you very much , sir . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "O: i do n't know . </S> <S> <S> <S> <S> <S>\n",
      "\n",
      "New Record!\n",
      "\n",
      "------------------------------\n",
      "Train: epoch 5\n",
      "Epoch:   5/10, Step:    1/6232, Loss:  3.786, Seconds: 64.03\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-250-cd1d00a84f2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mY_sent_lens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             dropout_keep_probability)\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-150-992f4520c597>\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, session, X, X_seq_len, Y, Y_seq_len, learning_rate, dropout_keep_probability)\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_predictions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             self.train_op], feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1276\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1263\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "all_model_predictions = []\n",
    "all_ground_truth = []\n",
    "\n",
    "checkpoint = \"model/2_trial/best_model.ckpt\"\n",
    "stop_early = 0\n",
    "stop = 5\n",
    "# validation_check = ((len(tokenized_questions_train))//batch_size//2)-1\n",
    "summary_test_loss = []\n",
    "\n",
    "train_set = list(zip(tokenized_questions_train, tokenized_answers_train))\n",
    "test_set = list(zip(tokenized_questions_test, tokenized_answers_test))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(test_set)\n",
    "       \n",
    "    print('-'*30)\n",
    "    print('Train: epoch', epoch + 1)\n",
    "    for n_iter, (X_batch, Y_batch) in enumerate(generate_batches(train_set, batch_size)):\n",
    "        start_time = time.time()      \n",
    "        X_ids, X_sent_lens = batch_to_ids(X_batch, word2id, max_len)\n",
    "        Y_ids, Y_sent_lens = batch_to_ids(Y_batch, word2id, max_len)\n",
    "        \n",
    "        predictions, loss = model.train_on_batch(\n",
    "            session,\n",
    "            X_ids,\n",
    "            X_sent_lens,\n",
    "            Y_ids,\n",
    "            Y_sent_lens,\n",
    "            learning_rate,\n",
    "            dropout_keep_probability)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        batch_time = end_time - start_time\n",
    "        if n_iter % 200 == 0:\n",
    "            print(\"Epoch: {:>3}/{}, Step: {:>4}/{}, Loss: {:>6.3f}, Seconds: {:>4.2f}\"\n",
    "                  .format(epoch+1, n_epochs, n_iter+1, n_step, loss, batch_time*200))\n",
    "            print('')\n",
    "#             print(\"Epoch: [%d/%d], step: [%d/%d], loss: %f\" % (epoch+1, n_epochs, n_iter+1, n_step, loss))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    epoch_test_loss = []\n",
    "    for n_iter, (X_batch, Y_batch) in enumerate(generate_batches(test_set, batch_size=batch_size)):        \n",
    "        X, X_sent_lens = batch_to_ids(X_sent, word2id, max_len)\n",
    "        Y, Y_sent_lens = batch_to_ids(Y_sent, word2id, max_len)\n",
    "\n",
    "        predictions, loss = model.predict_for_batch_with_loss(\n",
    "            session,\n",
    "            X,\n",
    "            X_sent_lens,\n",
    "            Y,\n",
    "            Y_sent_lens)\n",
    "        \n",
    "        epoch_test_loss.append(loss)\n",
    "        \n",
    "    end_time = time.time()\n",
    "    batch_time = end_time - start_time\n",
    "    print('Test: epoch', epoch+1, 'loss', np.mean(epoch_test_loss), 'Second:', batch_time)   \n",
    "    for x, y, p in list(zip(X, Y, predictions))[:3]:\n",
    "        print('X:', ' '.join(ids_to_sentence(x, id2word)))\n",
    "        print('Y:', ' '.join(ids_to_sentence(y, id2word)))\n",
    "        print('O:', ' '.join(ids_to_sentence(p, id2word)))\n",
    "        print('')\n",
    "    \n",
    "    # reduce learning rate\n",
    "    learning_rate *= learning_rate_decay\n",
    "    learning_rate = max(learning_rate, min_learning_rate)\n",
    "    \n",
    "    summary_test_loss.append(np.mean(epoch_test_loss))\n",
    "    if loss <= min(summary_test_loss):\n",
    "        print('New Record!')\n",
    "        print('')\n",
    "        stop_early = 0\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(session, checkpoint)\n",
    "    else:\n",
    "        print('No Improvement')\n",
    "        stop_early += 1\n",
    "        if stop_early == stop:\n",
    "            break\n",
    "            \n",
    "print('\\n...training finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: i do n't know . \n",
      "y: speed -- two hundred , seventy-five thousand kilometers per second . \n",
      "\n",
      "p: i do n't know . \n",
      "y: maybe later in the week . first i 've got to find myself a job . \n",
      "\n",
      "p: i do n't know . \n",
      "y: honest ? \n",
      "\n",
      "p: i 'm not sure you 're going to be able to get a chance . \n",
      "y: no , no you 're right , i 'm sorry . he uses women ; he lets them kill \n",
      "\n",
      "p: i 'm sorry . \n",
      "y: come in , come in . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "j=20\n",
    "num_display = 5\n",
    "for i in range(num_display):\n",
    "    print('p:', model_predictions[i+j])\n",
    "    print('y:', ground_truth[i+j])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention-based Encoder-Decoder Model Conclusion\n",
    "It toke almost 6 hrs to train, but only responses \"i'm sorry\", \"i don't know\"..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selective Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_train_path = 'data/cornell/questions_train.txt'\n",
    "answers_train_path = 'data/cornell/answers_train.txt'\n",
    "questions_test_path = 'data/cornell/questions_test.txt'\n",
    "answers_test_path = 'data/cornell/answers_test.txt'\n",
    "\n",
    "questions_train, answers_train, questions_test, answers_test = [], [], [], []\n",
    "\n",
    "dataset_list = [questions_train, answers_train, questions_test, answers_test]\n",
    "path_list = [questions_train_path, answers_train_path, questions_test_path, answers_test_path]\n",
    "\n",
    "for dataset, path in zip(dataset_list, path_list):\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            dataset.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_prepare(text):\n",
    "    \"\"\"Performs tokenization and simple preprocessing.\"\"\"\n",
    "    \n",
    "    replace_by_space_re = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "    bad_symbols_re = re.compile('[^0-9a-z #+_]')\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    tags_re = re.compile('<[^>]*>')\n",
    "    arrow_re = re.compile('[<>]')\n",
    "\n",
    "    text = text.lower()\n",
    "    text = replace_by_space_re.sub(' ', text)\n",
    "    text = bad_symbols_re.sub('', text)\n",
    "    text = tags_re.sub('', text)\n",
    "    text = arrow_re.sub('', text)\n",
    "    text = ' '.join([x for x in text.split() if x and x not in stopwords_set])\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_train = []\n",
    "question_ids = []\n",
    "for i in range(len(questions_train)):\n",
    "    question = text_prepare(questions_train[i])\n",
    "    answer = text_prepare(answers_train[i])\n",
    "    if ((len(question)>0) & (len(answer)>0)):        \n",
    "        question_answer_pair = question + '\\t' + answer\n",
    "        question_answer_train.append(question_answer_pair)\n",
    "        question_ids.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/cornell/question_answer_train.tsv'\n",
    "out = open(path, 'w')\n",
    "for line in question_answer_train:\n",
    "    line = line.strip().split('\\t')\n",
    "#     new_line = [text_prepare(q) for q in line]\n",
    "    print(*line, sep='\\t', file=out)\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: \n",
      "lr: 0.05\n",
      "dim: 100\n",
      "epoch: 5\n",
      "maxTrainTime: 8640000\n",
      "saveEveryEpoch: 0\n",
      "loss: hinge\n",
      "margin: 0.05\n",
      "similarity: cosine\n",
      "maxNegSamples: 10\n",
      "negSearchLimit: 10\n",
      "thread: 4\n",
      "minCount: 2\n",
      "minCountLabel: 1\n",
      "label: __label__\n",
      "ngrams: 1\n",
      "bucket: 2000000\n",
      "adagrad: 1\n",
      "trainMode: 3\n",
      "fileFormat: labelDoc\n",
      "normalizeText: 0\n",
      "dropoutLHS: 0\n",
      "dropoutRHS: 0\n",
      "Start to initialize starspace model.\n",
      "Build dict from input file : data/cornell/question_answer_train.tsv\n",
      "Read 2M words\n",
      "Number of words in dictionary:  49914\n",
      "Number of labels in dictionary: 0\n",
      "Loading data from file : data/cornell/question_answer_train.tsv\n",
      "Total number of examples loaded : 179611\n",
      "Initialized model weights. Model size :\n",
      "matrix : 49914 100\n",
      "Training epoch 0: 0.05 0.01\n",
      "Epoch: 100.0%  lr: 0.040000  loss: 0.077906  eta: 0h3m  tot: 0h0m45s  (20.0%)  lr: 0.049721  loss: 0.124236  eta: 0h4m  tot: 0h0m1s  (0.6%)48.9%  lr: 0.044525  loss: 0.087234  eta: 0h3m  tot: 0h0m22s  (9.8%)52.4%  lr: 0.044414  loss: 0.086244  eta: 0h3m  tot: 0h0m24s  (10.5%)54.4%  lr: 0.044246  loss: 0.086005  eta: 0h3m  tot: 0h0m25s  (10.9%)59.6%  lr: 0.043687  loss: 0.084487  eta: 0h3m  tot: 0h0m27s  (11.9%)85.7%  lr: 0.040838  loss: 0.079840  eta: 0h3m  tot: 0h0m39s  (17.1%)\n",
      " ---+++                Epoch    0 Train error : 0.07855152 +++--- ���\n",
      "Training epoch 1: 0.04 0.01\n",
      "Epoch: 100.0%  lr: 0.030000  loss: 0.020782  eta: 0h1m  tot: 0h1m22s  (40.0%)lr: 0.039944  loss: 0.020005  eta: 0h2m  tot: 0h0m46s  (20.3%)25.1%  lr: 0.037989  loss: 0.020838  eta: 0h2m  tot: 0h0m54s  (25.0%)25.9%  lr: 0.037877  loss: 0.020685  eta: 0h2m  tot: 0h0m55s  (25.2%)30.2%  lr: 0.037486  loss: 0.020613  eta: 0h2m  tot: 0h0m56s  (26.0%)52.1%  lr: 0.035252  loss: 0.020471  eta: 0h2m  tot: 0h1m5s  (30.4%)0.035196  loss: 0.020484  eta: 0h2m  tot: 0h1m5s  (30.7%)58.3%  lr: 0.034525  loss: 0.020480  eta: 0h2m  tot: 0h1m7s  (31.7%)68.1%  lr: 0.033240  loss: 0.020619  eta: 0h2m  tot: 0h1m11s  (33.6%)70.7%  lr: 0.032961  loss: 0.020631  eta: 0h2m  tot: 0h1m12s  (34.1%)\n",
      " ---+++                Epoch    1 Train error : 0.02060606 +++--- ���\n",
      "Training epoch 2: 0.03 0.01\n",
      "Epoch: 100.0%  lr: 0.020000  loss: 0.011979  eta: 0h1m  tot: 0h1m58s  (60.0%)5%  lr: 0.028883  loss: 0.011419  eta: 0h1m  tot: 0h1m26s  (41.9%)s  (44.3%)45.9%  lr: 0.025754  loss: 0.011630  eta: 0h1m  tot: 0h1m39s  (49.2%)77.7%  lr: 0.022179  loss: 0.011885  eta: 0h1m  tot: 0h1m50s  (55.5%)87.4%  lr: 0.020894  loss: 0.011957  eta: 0h1m  tot: 0h1m54s  (57.5%)\n",
      " ---+++                Epoch    2 Train error : 0.01189261 +++--- ���\n",
      "Training epoch 3: 0.02 0.01\n",
      "Epoch: 100.0%  lr: 0.010000  loss: 0.008788  eta: <1min   tot: 0h2m35s  (80.0%)  lr: 0.019777  loss: 0.008544  eta: 0h1m  tot: 0h1m59s  (60.5%)8.9%  lr: 0.019106  loss: 0.008577  eta: 0h1m  tot: 0h2m1s  (61.8%)11.7%  lr: 0.018994  loss: 0.008616  eta: 0h1m  tot: 0h2m2s  (62.3%)13.7%  lr: 0.018827  loss: 0.008515  eta: 0h1m  tot: 0h2m3s  (62.7%)24.1%  lr: 0.017654  loss: 0.008631  eta: 0h1m  tot: 0h2m6s  (64.8%)m7s  (65.1%)30.1%  lr: 0.016760  loss: 0.008696  eta: <1min   tot: 0h2m8s  (66.0%)38.9%  lr: 0.016033  loss: 0.008721  eta: <1min   tot: 0h2m12s  (67.8%)45.8%  lr: 0.015419  loss: 0.008728  eta: <1min   tot: 0h2m14s  (69.2%)48.8%  lr: 0.015028  loss: 0.008732  eta: <1min   tot: 0h2m15s  (69.8%)53.6%  lr: 0.014302  loss: 0.008756  eta: <1min   tot: 0h2m17s  (70.7%)61.5%  lr: 0.013799  loss: 0.008810  eta: <1min   tot: 0h2m19s  (72.3%)65.0%  lr: 0.013575  loss: 0.008782  eta: <1min   tot: 0h2m21s  (73.0%)67.5%  lr: 0.013352  loss: 0.008747  eta: <1min   tot: 0h2m21s  (73.5%)71.0%  lr: 0.013017  loss: 0.008723  eta: <1min   tot: 0h2m23s  (74.2%)83.2%  lr: 0.011452  loss: 0.008740  eta: <1min   tot: 0h2m28s  (76.6%)90.9%  lr: 0.010503  loss: 0.008768  eta: <1min   tot: 0h2m31s  (78.2%)s  (79.1%)  loss: 0.008794  eta: <1min   tot: 0h2m34s  (79.6%)\n",
      " ---+++                Epoch    3 Train error : 0.00875609 +++--- ���\n",
      "Training epoch 4: 0.01 0.01\n",
      "Epoch: 100.0%  lr: -0.000000  loss: 0.007295  eta: <1min   tot: 0h3m10s  (100.0%) (80.1%)0.7%  lr: 0.010000  loss: 0.007732  eta: <1min   tot: 0h2m35s  (80.1%)5.2%  lr: 0.009553  loss: 0.007656  eta: <1min   tot: 0h2m36s  (81.0%)13.9%  lr: 0.008603  loss: 0.007705  eta: <1min   tot: 0h2m39s  (82.8%)18.9%  lr: 0.008045  loss: 0.007569  eta: <1min   tot: 0h2m41s  (83.8%)28.1%  lr: 0.007374  loss: 0.007434  eta: <1min   tot: 0h2m44s  (85.6%)44.9%  lr: 0.004916  loss: 0.007311  eta: <1min   tot: 0h2m50s  (89.0%)49.1%  lr: 0.004637  loss: 0.007318  eta: <1min   tot: 0h2m51s  (89.8%)56.0%  lr: 0.003799  loss: 0.007349  eta: <1min   tot: 0h2m54s  (91.2%)63.1%  lr: 0.003128  loss: 0.007402  eta: <1min   tot: 0h2m56s  (92.6%)70.3%  lr: 0.002626  loss: 0.007357  eta: <1min   tot: 0h2m59s  (94.1%)72.7%  lr: 0.002346  loss: 0.007364  eta: <1min   tot: 0h2m59s  (94.5%)74.5%  lr: 0.002123  loss: 0.007344  eta: <1min   tot: 0h3m0s  (94.9%)\n",
      " ---+++                Epoch    4 Train error : 0.00731799 +++--- ���\n",
      "Saving model to file : starspace_embedding\n",
      "Saving model in tsv format : starspace_embedding.tsv\n"
     ]
    }
   ],
   "source": [
    "!starspace train -trainFile \"data/cornell/question_answer_train.tsv\" -model starspace_embedding \\\n",
    "-trainMode 3 \\\n",
    "-adagrad true \\\n",
    "-ngrams 1 \\\n",
    "-epoch 5 \\\n",
    "-dim 100 \\\n",
    "-similarity \"cosine\" \\\n",
    "-minCount 2 \\\n",
    "-verbose true \\\n",
    "-fileFormat labelDoc \\\n",
    "-negSearchLimit 10 \\\n",
    "-lr 0.05 \\\n",
    "-thread 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(embeddings_path):\n",
    "    embeddings = {}\n",
    "    for line in open(embeddings_path):\n",
    "        word, *arr = line.split('\\t')\n",
    "        embeddings[word] = np.asarray(arr, dtype='float32')\n",
    "        \n",
    "    dim = len(arr)\n",
    "    \n",
    "    return embeddings, dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "starspace_embeddings, embeddings_dim = load_embeddings('starspace_embedding.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec(question, embeddings, dim):\n",
    "    question2vec = [embeddings[word] for word in question.split() if word in embeddings]\n",
    "    \n",
    "    if not question2vec:\n",
    "        return np.zeros(dim)\n",
    "    \n",
    "    question2vec = np.array(question2vec)\n",
    "    \n",
    "    return question2vec.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "question_matrix = np.zeros((len(question_answer_train), embeddings_dim), dtype=np.float32)\n",
    "\n",
    "for i, question in enumerate(question_answer_train):\n",
    "    question = question.split('\\t')[0]\n",
    "    question_matrix[i, :] = question_to_vec(question, starspace_embeddings, embeddings_dim)\n",
    "file_name = 'question_matrix.pkl'\n",
    "pickle.dump(question_matrix, open(file_name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_pairs = []\n",
    "for q_id in question_ids:\n",
    "    answer_pairs.append(answers_train[q_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_pairs_path = 'answer_pair.txt'\n",
    "out = open(answer_pairs_path, 'w')\n",
    "for line in answer_pairs:\n",
    "    print(line, sep='\\t', file=out)\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
    "\n",
    "def get_best_answer(question, word_embeddings, embeddings_dim, question_matrix, answer_pairs):\n",
    "        \"\"\" Returns id of the most similar thread for the question.\n",
    "            The search is performed across the threads with a given tag.\n",
    "        \"\"\"\n",
    "        # HINT: you have already implemented a similar routine in the 3rd assignment.\n",
    "        \n",
    "        question = text_prepare(question)\n",
    "        question_vec = question_to_vec(question, word_embeddings, embeddings_dim)\n",
    "        best_answer_id = pairwise_distances_argmin(question_vec.reshape(1, -1), question_matrix)[0]\n",
    "        \n",
    "        return answer_pairs[best_answer_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Selective Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: hi\n",
      "A: We had a date.\n",
      "\n",
      "Q: nice to meet you\n",
      "A: Peace out, Craig.\n",
      "\n",
      "Q: good to see you\n",
      "A: Where's Plissken?\n",
      "\n",
      "Q: let's go party\n",
      "A: Of course, birthday and welcome home... who'll I ask?\n",
      "\n",
      "Q: bye\n",
      "A: Bye.\n",
      "\n",
      "Q: who are you?\n",
      "A: What'd you see, who was she with, where were they going?\n",
      "\n",
      "Q: get out\n",
      "A: I'm sure it won't be long now.\n",
      "\n",
      "Q: It's imperative\n",
      "A: Just cover me. It was built to move.\n",
      "\n",
      "Q: why so serius?\n",
      "A: What'd you see, who was she with, where were they going?\n",
      "\n",
      "Q: Where did he go?\n",
      "A: Okay.\n",
      "\n",
      "Q: What does it cost?\n",
      "A: You want sophistication, it don't come cheap.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\"hi\", \"nice to meet you\", \"good to see you\", \"let's go party\",\n",
    "             \"bye\", \"who are you?\", \"get out\", \"It's imperative\",\n",
    "             \"why so serius?\", \"Where did he go?\", \"What does it cost?\"]\n",
    "\n",
    "for q in questions:\n",
    "    answer = get_best_answer(q, starspace_embeddings, embeddings_dim, question_matrix, answer_pairs)\n",
    "    print('Q:', q)\n",
    "    print('A:', answer)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclustion\n",
    "The model sometimes responses weirdly, but still much better than the Attention-based Seq2Seq model above. And it didn't take so long to train the Starspace word embeddings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
